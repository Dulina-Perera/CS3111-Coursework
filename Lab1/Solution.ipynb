{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the Deprecation Warnings.\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Load in the necessary libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset.\n",
    "df_train = pd.read_csv('Resources/train.csv')\n",
    "df_val = pd.read_csv('Resources/valid.csv')\n",
    "df_test = pd.read_csv('Resources/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a general overview of the dataset.\n",
    "display(df_train.head())\n",
    "display(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns where more than 50% of the data is missing.\n",
    "df_train.dropna(axis='columns', inplace=True, thresh=len(df_train)/2)\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting and Removing Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.select_dtypes(include=['int64', 'float64']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The business context should govern how we define and react to outliers.\n",
    "# The meanings of our findings should be dictated by the underlying context, rather than the number itself.\n",
    "\n",
    "sns.boxplot(x=df_train['int_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def scale_features(df):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original DataFrame.\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    # Lowercase categorical columns.\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    df_scaled[categorical_columns] = df_scaled[categorical_columns].apply(lambda x: x.str.lower())\n",
    "\n",
    "    # Encode categorical columns using LabelEncoder.\n",
    "    label_encoders = {}\n",
    "    for column in categorical_columns:\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        df_scaled[column] = label_encoders[column].fit_transform(df_scaled[column])\n",
    "\n",
    "    # Scale numerical columns using StandardScaler, excluding the target column if it exists.\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_columns = df_scaled.select_dtypes(include=['int', 'float']).columns\n",
    "    if 'loan_status' in numerical_columns:\n",
    "        numerical_columns = numerical_columns.drop('loan_status')\n",
    "    df_scaled[numerical_columns] = scaler.fit_transform(df_scaled[numerical_columns])\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "# Apply the preprocess_data function to each dataset\n",
    "df_scaled = scale_features(df_train)\n",
    "df_scaled_val = scale_features(df_val)\n",
    "df_scaled_test = scale_features(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Train the classifier on the entire dataset\n",
    "xgb_classifier.fit(df_scaled.drop('loan_status', axis=1), df_scaled['loan_status'])\n",
    "\n",
    "# Use feature importances to select top features\n",
    "feature_importances = xgb_classifier.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order and get corresponding indices\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Select top k features based on importance scores\n",
    "top_k = 4  # You can adjust this value as needed\n",
    "selected_feature_indices = sorted_indices[:top_k]\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = df_scaled.drop('loan_status', axis=1).columns[selected_feature_indices]\n",
    "\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Subset the DataFrame with selected features for training\n",
    "X_selected_train = df_scaled[selected_features]\n",
    "y_train = df_scaled['loan_status']\n",
    "\n",
    "# Subset the DataFrame with selected features for validation\n",
    "X_selected_val = df_scaled_val[selected_features]\n",
    "y_val = df_scaled_val['loan_status']\n",
    "\n",
    "# Initialize XGBoost classifier with default hyperparameters\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Train the classifier on the selected features using training data\n",
    "xgb_classifier.fit(X_selected_train, y_train)\n",
    "\n",
    "# Predict on the validation dataset\n",
    "y_pred_val = xgb_classifier.predict(X_selected_val)\n",
    "\n",
    "# Evaluate the classifier on the validation dataset\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val, end='\\n\\n\\n\\n')\n",
    "\n",
    "# Print classification report for validation dataset\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Now, predict the target variable for df_scaled_test\n",
    "# # Subset the DataFrame with selected features for testing\n",
    "# X_selected_test = df_scaled_test[selected_features]\n",
    "\n",
    "# # Predict on the test dataset\n",
    "# y_pred_test = xgb_classifier.predict(X_selected_test)\n",
    "\n",
    "# # Create a new DataFrame with selected features and predicted target variable\n",
    "# df_test_with_predictions = X_selected_test.copy()\n",
    "# df_test_with_predictions['loan_status'] = y_pred_test  # Add predicted target variable to DataFrame\n",
    "\n",
    "# # Rearrange columns to have predicted_loan_status as the first column\n",
    "# df_test_with_predictions = df_test_with_predictions[['loan_status'] + list(X_selected_test.columns)]\n",
    "\n",
    "# # Save the DataFrame to a new CSV file\n",
    "# df_test_with_predictions.to_csv('210465P.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
