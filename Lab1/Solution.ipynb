{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the Deprecation Warnings.\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Load in the necessary libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset.\n",
    "df_train = pd.read_csv('Resources/train.csv')\n",
    "df_val = pd.read_csv('Resources/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a general overview of the dataset.\n",
    "print(df_train.head(), end='\\n\\n')\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In data cleaning several tasks are needed to be done.\n",
    "\n",
    "  1. **Handle missing values.**\n",
    "  2. **Smoothen noisy data.**\n",
    "  3. **Identify and remove outliers.**\n",
    "  4. **Correct inconsistencies.**\n",
    "  5. **Resolve redundancies caused by data integration.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of missing data points per column.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "missing_values_count = df_train.isnull().sum()\n",
    "print(missing_values_count)\n",
    "\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517788, 87)\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns where more than 50% of the data is missing.\n",
    "df_train.dropna(axis='columns', inplace=True, thresh=len(df_train)/2)\n",
    "# df_val.dropna(axis='columns', inplace=True, thresh=len(df_val)/2)\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the categorical columns with missing data.\n",
    "categorical_nan_columns = [col for col in df_train.columns if df_train[col].dtype == 'object' and df_train[col].isnull().any()]\n",
    "print(categorical_nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certain categorical columns can be automatically filled.\n",
    "df_train.fillna(value={'emp_title': 'unemployed'}, inplace=True)\n",
    "df_train.fillna(value={'emp_length': '< 1 year'}, inplace=True)\n",
    "df_train.fillna(value={'title': 'not provided'}, inplace=True)\n",
    "df_train.fillna(value={'zip_code': 'unknown'}, inplace=True)\n",
    "\n",
    "\n",
    "df_val.fillna(value={'emp_title': 'unemployed'}, inplace=True)\n",
    "df_val.fillna(value={'emp_length': '< 1 year'}, inplace=True)\n",
    "df_val.fillna(value={'title': 'not provided'}, inplace=True)\n",
    "df_val.fillna(value={'zip_code': 'unknown'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'last_pymnt_d' to datetime format.\n",
    "df_train['last_pymnt_d'] = pd.to_datetime(df_train['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# Extract year and month.\n",
    "df_train['last_pymnt_year'] = df_train['last_pymnt_d'].dt.year\n",
    "df_train['last_pymnt_month'] = df_train['last_pymnt_d'].dt.month\n",
    "\n",
    "# Fill NaN values.\n",
    "df_train['last_pymnt_year'].fillna(9999, inplace=True)\n",
    "df_train['last_pymnt_month'].fillna(99, inplace=True)\n",
    "\n",
    "# Convert year and month to int as they might be float due to NaNs.\n",
    "df_train['last_pymnt_year'] = df_train['last_pymnt_year'].astype(int)\n",
    "df_train['last_pymnt_month'] = df_train['last_pymnt_month'].astype(int)\n",
    "\n",
    "# Drop the 'last_pymnt_d' column.\n",
    "df_train.drop('last_pymnt_d', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Convert 'last_pymnt_d' to datetime format.\n",
    "df_val['last_pymnt_d'] = pd.to_datetime(df_val['last_pymnt_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# Extract year and month.\n",
    "df_val['last_pymnt_year'] = df_val['last_pymnt_d'].dt.year\n",
    "df_val['last_pymnt_month'] = df_val['last_pymnt_d'].dt.month\n",
    "\n",
    "# Fill NaN values.\n",
    "df_val['last_pymnt_year'].fillna(9999, inplace=True)\n",
    "df_val['last_pymnt_month'].fillna(99, inplace=True)\n",
    "\n",
    "# Convert year and month to int as they might be float due to NaNs.\n",
    "df_val['last_pymnt_year'] = df_val['last_pymnt_year'].astype(int)\n",
    "df_val['last_pymnt_month'] = df_val['last_pymnt_month'].astype(int)\n",
    "\n",
    "# Drop the 'last_pymnt_d' column.\n",
    "df_val.drop('last_pymnt_d', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'last_pymnt_d' to datetime format.\n",
    "df_train['last_credit_pull_d'] = pd.to_datetime(df_train['last_credit_pull_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# Extract year and month.\n",
    "df_train['last_credit_pull_year'] = df_train['last_credit_pull_d'].dt.year\n",
    "df_train['last_credit_pull_month'] = df_train['last_credit_pull_d'].dt.month\n",
    "\n",
    "# Fill NaN values.\n",
    "df_train['last_credit_pull_year'].fillna(0000, inplace=True)\n",
    "df_train['last_credit_pull_month'].fillna(00, inplace=True)\n",
    "\n",
    "# Convert year and month to int as they might be float due to NaNs.\n",
    "df_train['last_credit_pull_year'] = df_train['last_credit_pull_year'].astype(int)\n",
    "df_train['last_credit_pull_month'] = df_train['last_credit_pull_month'].astype(int)\n",
    "\n",
    "# Drop the 'last_pymnt_d' column.\n",
    "df_train.drop('last_credit_pull_d', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Convert 'last_pymnt_d' to datetime format.\n",
    "df_val['last_credit_pull_d'] = pd.to_datetime(df_val['last_credit_pull_d'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# Extract year and month.\n",
    "df_val['last_credit_pull_year'] = df_val['last_credit_pull_d'].dt.year\n",
    "df_val['last_credit_pull_month'] = df_val['last_credit_pull_d'].dt.month\n",
    "\n",
    "# Fill NaN values.\n",
    "df_val['last_credit_pull_year'].fillna(0000, inplace=True)\n",
    "df_val['last_credit_pull_month'].fillna(00, inplace=True)\n",
    "\n",
    "# Convert year and month to int as they might be float due to NaNs.\n",
    "df_val['last_credit_pull_year'] = df_val['last_credit_pull_year'].astype(int)\n",
    "df_val['last_credit_pull_month'] = df_val['last_credit_pull_month'].astype(int)\n",
    "\n",
    "# Drop the 'last_pymnt_d' column.\n",
    "df_val.drop('last_credit_pull_d', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now drop all other rows with missing values.\n",
    "df_train.dropna(axis='rows', inplace=True)\n",
    "\n",
    "\n",
    "df_val.dropna(axis='rows', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "- If features are not Gaussian-like, say, has a skewed distribution or has outliers, Normalization - Standardization is not a good choice as it will compress most data to a narrow range.\n",
    "\n",
    "- However, we can transform the feature into Gaussian like and then use Normalization - Standardization.\n",
    "\n",
    "- When performing distance or covariance calculation (algorithm like Clustering, PCA and LDA), it is better to use Normalization - Standardization as it will remove the effect of scales on variance and covariance.\n",
    "\n",
    "- Min-Max scaling has the same drawbacks as Normalization - Standardization, and also new data may not be bounded to [0,1] as they can be out of the original range. Some algorithms, for example some deep learning network prefer input on a 0-1 scale so this is a good choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.kdeplot(df_train['loan_amnt'], color='skyblue', fill=True)\n",
    "\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_amnt  funded_amnt  funded_amnt_inv  term  int_rate  installment  \\\n",
      "0   0.341772     0.341772          0.35000     0  0.079439     0.251156   \n",
      "1   0.037975     0.037975          0.05000     0  0.427570     0.038398   \n",
      "2   0.113924     0.113924          0.11875     0  0.080997     0.087899   \n",
      "3   0.508861     0.508861          0.51500     0  0.260125     0.397073   \n",
      "4   0.240506     0.240506          0.25000     0  0.080997     0.178680   \n",
      "\n",
      "   grade  sub_grade  emp_title  emp_length  ...  pub_rec_bankruptcies  \\\n",
      "0      0          3      96610           0  ...              0.000000   \n",
      "1      3         16     175168          11  ...              0.083333   \n",
      "2      0          3     166043           4  ...              0.000000   \n",
      "3      1          9     175168          11  ...              0.000000   \n",
      "4      0          3     122003          10  ...              0.000000   \n",
      "\n",
      "   tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
      "0   0.022222         0.006794           0.008947        0.024251   \n",
      "1   0.000000         0.000980           0.001576        0.002318   \n",
      "2   0.000000         0.023780           0.007586        0.114479   \n",
      "3   0.000000         0.010510           0.004672        0.027282   \n",
      "4   0.022222         0.013574           0.053650        0.021398   \n",
      "\n",
      "   total_il_high_credit_limit  hardship_flag  disbursement_method  \\\n",
      "0                    0.024021              0                    0   \n",
      "1                    0.000000              0                    0   \n",
      "2                    0.010800              0                    0   \n",
      "3                    0.000000              0                    0   \n",
      "4                    0.061767              0                    0   \n",
      "\n",
      "   debt_settlement_flag  loan_status  \n",
      "0                     0          1.0  \n",
      "1                     0          1.0  \n",
      "2                     0          1.0  \n",
      "3                     0          1.0  \n",
      "4                     0          1.0  \n",
      "\n",
      "[5 rows x 87 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulina-perera/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "/home/dulina-perera/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  term  int_rate  \\\n",
      "0 NaN        NaN   0.356329     0.356329         0.364375     1  0.477414   \n",
      "1 NaN        NaN   0.240506     0.240506         0.250000     0  0.322430   \n",
      "2 NaN        NaN   0.417722     0.417722         0.425000     0  0.313084   \n",
      "3 NaN        NaN   0.101266     0.101266         0.112500     0  0.179907   \n",
      "4 NaN        NaN   0.145570     0.145570         0.156250     0  0.299065   \n",
      "\n",
      "   installment  grade  sub_grade  ...  hardship_last_payment_amount  \\\n",
      "0     0.206440      3         16  ...                           NaN   \n",
      "1     0.190590      2         11  ...                           NaN   \n",
      "2     0.329469      2         11  ...                           NaN   \n",
      "3     0.075939      1          6  ...                           NaN   \n",
      "4     0.114504      1          8  ...                           NaN   \n",
      "\n",
      "   disbursement_method  debt_settlement_flag  debt_settlement_flag_date  \\\n",
      "0                    0                     0                         69   \n",
      "1                    0                     0                         69   \n",
      "2                    0                     0                         69   \n",
      "3                    0                     0                         69   \n",
      "4                    0                     0                         69   \n",
      "\n",
      "   settlement_status  settlement_date  settlement_amount  \\\n",
      "0                  3               77                NaN   \n",
      "1                  3               77                NaN   \n",
      "2                  3               77                NaN   \n",
      "3                  3               77                NaN   \n",
      "4                  3               77                NaN   \n",
      "\n",
      "   settlement_percentage  settlement_term  loan_status  \n",
      "0                    NaN              NaN          1.0  \n",
      "1                    NaN              NaN          1.0  \n",
      "2                    NaN              NaN          1.0  \n",
      "3                    NaN              NaN          1.0  \n",
      "4                    NaN              NaN          1.0  \n",
      "\n",
      "[5 rows x 145 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# df['term'] = le.fit_transform(df['term'])\n",
    "# Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "df_scaled = df_train.copy()\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in df_train.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df_scaled[column] = label_encoders[column].fit_transform(df_train[column])\n",
    "\n",
    "# Scale numerical columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = df_train.select_dtypes(include=['int', 'float']).columns\n",
    "df_scaled[numerical_columns] = scaler.fit_transform(df_train[numerical_columns])\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(df_scaled.head())\n",
    "\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# df['term'] = le.fit_transform(df['term'])\n",
    "# Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "df_scaled_val = df_val.copy()\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "label_encoders = {}\n",
    "for column in df_val.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df_scaled_val[column] = label_encoders[column].fit_transform(df_val[column])\n",
    "\n",
    "# Scale numerical columns using MinMaxScaler\n",
    "numerical_columns = df_val.select_dtypes(include=['int', 'float']).columns\n",
    "df_scaled_val[numerical_columns] = scaler.fit_transform(df_val[numerical_columns])\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(df_scaled_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Perform feature selection with SelectFromModel\n",
    "selector = SelectFromModel(xgb_classifier, threshold=-np.inf, max_features=10)  # Select top 10 features\n",
    "selector.fit(df_scaled.drop('loan_status', axis=1), df_scaled['loan_status'])\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_names = df_scaled.drop('loan_status', axis=1).columns[selected_feature_indices]\n",
    "\n",
    "# Subset the DataFrame with selected features\n",
    "X_selected = df_scaled[selected_feature_names]\n",
    "\n",
    "# Now you can proceed with model training using X_selected and df_scaled['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of selected features\n",
    "selected_indices = selector.get_support()\n",
    "\n",
    "# Get the names of selected features\n",
    "selected_features = df_scaled.drop('loan_status', axis=1).columns[selected_indices]\n",
    "\n",
    "# Print the names of selected features\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9995828408537857\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     52331\n",
      "         1.0       1.00      1.00      1.00    120265\n",
      "\n",
      "    accuracy                           1.00    172596\n",
      "   macro avg       1.00      1.00      1.00    172596\n",
      "weighted avg       1.00      1.00      1.00    172596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Perform feature selection with SelectFromModel\n",
    "selector = SelectFromModel(xgb_classifier, threshold=-np.inf, max_features=10)  # Select top 10 features\n",
    "selector.fit(df_scaled.drop('loan_status', axis=1), df_scaled['loan_status'])\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_names = df_scaled.drop('loan_status', axis=1).columns[selected_feature_indices]\n",
    "\n",
    "# Subset the DataFrame with selected features for training\n",
    "X_selected_train = df_scaled[selected_feature_names]\n",
    "y_train = df_scaled['loan_status']\n",
    "\n",
    "# Subset the DataFrame with selected features for validation\n",
    "X_selected_val = df_scaled_val[selected_feature_names]\n",
    "y_val = df_scaled_val['loan_status']\n",
    "\n",
    "# Initialize XGBoost classifier with default hyperparameters\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Train the classifier on the selected features using training data\n",
    "xgb_classifier.fit(X_selected_train, y_train)\n",
    "\n",
    "# Predict on the validation dataset\n",
    "y_pred_val = xgb_classifier.predict(X_selected_val)\n",
    "\n",
    "# Evaluate the classifier on the validation dataset\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(\"Validation Accuracy:\", accuracy_val)\n",
    "\n",
    "# Print classification report for validation dataset\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
